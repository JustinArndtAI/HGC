{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RNN training...\n",
      "Epoch 1/5, Loss: 1.4594\n",
      "Epoch 2/5, Loss: 1.4896\n",
      "Epoch 3/5, Loss: 1.4587\n",
      "Epoch 4/5, Loss: 1.4407\n",
      "Epoch 5/5, Loss: 1.4409\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# Load small text corpus (tiny Shakespeare)\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = urllib.request.urlopen(url)\n",
    "text = response.read().decode('utf-8')\n",
    "\n",
    "# Preprocess the text\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "# Define dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.data[idx:idx+self.seq_len]),\n",
    "            torch.tensor(self.data[idx+1:idx+self.seq_len+1])\n",
    "        )\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "seq_len = 100\n",
    "dataset = CharDataset(data, seq_len)\n",
    "# THE FIX: Set drop_last=True to avoid smaller, problematic batches\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# Define simple character-level RNN\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "model = CharRNN(vocab_size, hidden_size, vocab_size, n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print('Starting RNN training...')\n",
    "n_epochs = 5 # Reduced for quicker validation\n",
    "for epoch in range(n_epochs):\n",
    "    # THE FIX: Hidden state is now initialized correctly inside the loop\n",
    "    for x, y in dataloader:\n",
    "        hidden = model.init_hidden(BATCH_SIZE) # Initialize with the constant batch size\n",
    "        \n",
    "        outputs, hidden = model(x, hidden)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), y.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}')\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Generation: To be or not to be,-hating must as stilded king, break of the callide I honourse\n",
      "have winds: and my our point.\n",
      "\n",
      "BENVOLIO:\n",
      "Up that I say, alonio thee,\n",
      "And on she speve not him.\n",
      "\n",
      "Third Anmator not, cure,\n",
      "Send,\n",
      "incelved, \n",
      "QIEP Generation: To be or not to beaieooieaoiiioaeiaiiiooiooeaiiiiiiaueeoeeaieoeeaooieoiaiaeaeiaeeioioooioiieioeoaiioaieieiaoiiiaieoiiiaeaoeaiieiieieaaeiiiieaeaiioiieeioeaiieioueoieeeiaeieeoiiiiaeoiiooiaeiaeaeeoeaeooooiiiiiuieioaiiauei\n"
     ]
    }
   ],
   "source": [
    "def prune_and_sample(logits, threshold=0.1):\n",
    "    # Assuming logits is the output of the linear layer (batch_size, seq_len, vocab_size)\n",
    "    if logits.dim() > 1:\n",
    "        logits = logits[-1]\n",
    "    # Treat logits as a matrix for SVD (though it's 1D, reshape if needed; here assume transition tensor is fc.weight)\n",
    "    transition = model.fc.weight.data  # (vocab_size, hidden_size)\n",
    "    U, S, Vh = torch.svd(transition)\n",
    "    # Prune low singular values\n",
    "    mask = S > (threshold * S.max())\n",
    "    S_pruned = S * mask.float()\n",
    "    transition_pruned = U @ torch.diag(S_pruned) @ Vh.T\n",
    "    # Recompute logits with pruned transition (simplified; in practice, apply to hidden state)\n",
    "    hidden_last = hidden[-1, -1]  # Last hidden state\n",
    "    logits_pruned = hidden_last @ transition_pruned.T + model.fc.bias\n",
    "    probs = torch.softmax(logits_pruned, dim=0)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_text(model, seed, length=200, use_qiep=False):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    generated = [char_to_idx[ch] for ch in seed]\n",
    "    for ch in generated[:-1]:\n",
    "        output, hidden = model(torch.tensor([[ch]]), hidden)\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(torch.tensor([[generated[-1]]]), hidden)\n",
    "        if use_qiep:\n",
    "            next_char = prune_and_sample(output[0])\n",
    "        else:\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_char)\n",
    "    return ''.join([idx_to_char[i] for i in generated])\n",
    "\n",
    "# Generate samples\n",
    "seed = 'To be or not to be'\n",
    "standard_text = generate_text(model, seed, use_qiep=False)\n",
    "qiep_text = generate_text(model, seed, use_qiep=True)\n",
    "print('Standard Generation:', standard_text)\n",
    "print('QIEP Generation:', qiep_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Average Entropy: 2.1177\n",
      "QIEP Average Entropy: 2.0414\n"
     ]
    }
   ],
   "source": [
    "def shannon_entropy(probs):\n",
    "    return -torch.sum(probs * torch.log2(probs + 1e-10))\n",
    "\n",
    "def compute_average_entropy(model, seed, num_steps=100, use_qiep=False):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    generated = [char_to_idx[ch] for ch in seed]\n",
    "    for ch in generated[:-1]:\n",
    "        output, hidden = model(torch.tensor([[ch]]), hidden)\n",
    "    entropies = []\n",
    "    for _ in range(num_steps):\n",
    "        output, hidden = model(torch.tensor([[generated[-1]]]), hidden)\n",
    "        if use_qiep:\n",
    "            # Recompute pruned logits\n",
    "            transition = model.fc.weight.data\n",
    "            U, S, Vh = torch.svd(transition)\n",
    "            mask = S > (0.1 * S.max())\n",
    "            S_pruned = S * mask.float()\n",
    "            transition_pruned = U @ torch.diag(S_pruned) @ Vh.T\n",
    "            hidden_last = hidden[-1, -1]\n",
    "            logits_pruned = hidden_last @ transition_pruned.T + model.fc.bias\n",
    "            probs = torch.softmax(logits_pruned, dim=0)\n",
    "        else:\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "        entropy = shannon_entropy(probs)\n",
    "        entropies.append(entropy.item())\n",
    "        # Sample next char to continue (but we don't use it for entropy calc)\n",
    "        next_char = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_char)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "seed = 'To be or not to be'\n",
    "standard_entropy = compute_average_entropy(model, seed, use_qiep=False)\n",
    "qiep_entropy = compute_average_entropy(model, seed, use_qiep=True)\n",
    "print(f'Standard Average Entropy: {standard_entropy:.4f}')\n",
    "print(f'QIEP Average Entropy: {qiep_entropy:.4f}')\n",
    "assert qiep_entropy < standard_entropy, 'QIEP should have lower entropy'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
