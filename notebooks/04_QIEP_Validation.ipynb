{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# Load small text corpus (tiny Shakespeare)\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = urllib.request.urlopen(url)\n",
    "text = response.read().decode('utf-8')\n",
    "\n",
    "# Preprocess the text\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "# Define dataset\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.data[idx:idx+self.seq_len]),\n",
    "            torch.tensor(self.data[idx+1:idx+self.seq_len+1])\n",
    "        )\n",
    "\n",
    "seq_len = 100\n",
    "dataset = CharDataset(data, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define simple character-level RNN\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "model = CharRNN(vocab_size, hidden_size, vocab_size, n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 10  # Small number for laptop-scale\n",
    "for epoch in range(n_epochs):\n",
    "    hidden = model.init_hidden(64)\n",
    "    for x, y in dataloader:\n",
    "        hidden = hidden.detach()\n",
    "        outputs, hidden = model(x, hidden)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_and_sample(logits, threshold=0.1):\n",
    "    # Assuming logits is the output of the linear layer (batch_size, seq_len, vocab_size)\n",
    "    # For simplicity, process last token's logits (1, vocab_size)\n",
    "    if logits.dim() > 1:\n",
    "        logits = logits[-1]\n",
    "    # Treat logits as a matrix for SVD (though it's 1D, reshape if needed; here assume transition tensor is fc.weight)\n",
    "    # For demo, use SVD on fc.weight as transition tensor\n",
    "    transition = model.fc.weight.data  # (vocab_size, hidden_size)\n",
    "    U, S, Vh = torch.svd(transition)\n",
    "    # Prune low singular values\n",
    "    mask = S > (threshold * S.max())\n",
    "    S_pruned = S * mask.float()\n",
    "    transition_pruned = U @ torch.diag(S_pruned) @ Vh.T\n",
    "    # Recompute logits with pruned transition (simplified; in practice, apply to hidden state)\n",
    "    hidden_last = hidden[-1, -1]  # Last hidden state\n",
    "    logits_pruned = hidden_last @ transition_pruned.T + model.fc.bias\n",
    "    probs = torch.softmax(logits_pruned, dim=0)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_text(model, seed, length=200, use_qiep=False):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    generated = [char_to_idx[ch] for ch in seed]\n",
    "    for ch in generated[:-1]:\n",
    "        output, hidden = model(torch.tensor([[ch]]), hidden)\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(torch.tensor([[generated[-1]]]), hidden)\n",
    "        if use_qiep:\n",
    "            next_char = prune_and_sample(output[0])\n",
    "        else:\n",
    "            probs = torch.softmax(output[0, -1], dim=0)\n",
    "            next_char = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_char)\n",
    "    return ''.join([idx_to_char[i] for i in generated])\n",
    "\n",
    "# Generate samples\n",
    "seed = 'To be or not to be'\n",
    "standard_text = generate_text(model, seed, use_qiep=False)\n",
    "qiep_text = generate_text(model, seed, use_qiep=True)\n",
    "print('Standard Generation:', standard_text)\n",
    "print('QIEP Generation:', qiep_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
