{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "print('All libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom HGC classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# === ALL CLASS DEFINITIONS ARE NOW INSIDE THE NOTEBOOK ===\n",
    "\n",
    "class VSA:\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "    def bind(self, vec1: np.ndarray, vec2: np.ndarray) -> np.ndarray:\n",
    "        fft1 = np.fft.fft(vec1)\n",
    "        fft2 = np.fft.fft(vec2)\n",
    "        return np.fft.ifft(fft1 * fft2).real\n",
    "    def unbind(self, vec1: np.ndarray, vec2: np.ndarray) -> np.ndarray:\n",
    "        fft1 = np.fft.fft(vec1)\n",
    "        fft2 = np.fft.fft(vec2)\n",
    "        return np.fft.ifft(fft1.conj() * fft2).real\n",
    "\n",
    "class HGC_Layer(nn.Module):\n",
    "    def __init__(self, hidden_size, hkm_dim=2048):\n",
    "        super().__init__()\n",
    "        self.hkm_dim = hkm_dim\n",
    "        self.query_projection = nn.Linear(hidden_size, hkm_dim)\n",
    "        self.hkm = nn.Parameter(torch.randn(1, hkm_dim))\n",
    "    def forward(self, hidden_states):\n",
    "        queries = self.query_projection(hidden_states)\n",
    "        # Simplified interaction with the HKM\n",
    "        return torch.tanh(queries @ self.hkm.T @ self.hkm)\n",
    "\n",
    "class GPT2_With_HGC(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.hgc_layer = HGC_Layer(hidden_size=config.n_embd, hkm_dim=2048)\n",
    "        self.hgc_output_projection = nn.Linear(self.hgc_layer.hkm_dim, config.n_embd)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # Pass inputs to the main transformer body\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        \n",
    "        # HGC-specific logic\n",
    "        hgc_info = self.hgc_layer(hidden_states)\n",
    "        projected_hgc_info = self.hgc_output_projection(hgc_info)\n",
    "        conditioned_hidden_states = hidden_states + projected_hgc_info\n",
    "        \n",
    "        # Get final logits\n",
    "        lm_logits = self.lm_head(conditioned_hidden_states)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "        # THE FIX: Ensure the loss is included in the output object\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "        )\n",
    "\n",
    "print('Custom HGC classes defined successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading and Preparation ---\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset('truthful_qa', 'generation')\n",
    "train_test = dataset['validation'].train_test_split(test_size=0.2, seed=42)\n",
    "def format_and_tokenize(examples):\n",
    "    formatted_texts = [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(examples['question'], examples['best_answer'])]\n",
    "    return tokenizer(formatted_texts, truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_train = train_test['train'].map(format_and_tokenize, batched=True, remove_columns=train_test['train'].column_names)\n",
    "tokenized_test = train_test['test'].map(format_and_tokenize, batched=True, remove_columns=train_test['test'].column_names)\n",
    "tokenized_train.set_format(type='torch')\n",
    "tokenized_test.set_format(type='torch')\n",
    "print('Data prepared.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained GPT-2 model...\n",
      "Initializing custom HGC model architecture...\n",
      "Transferring weights to HGC model...\n",
      "Weight transfer complete.\n",
      "Starting fine-tuning of HGC model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.193300</td>\n",
       "      <td>2.083067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the HGC model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HGC Model Perplexity: 8.03\n",
      "Saving HGC model to C:/HGC/models/hgc_gpt2_truthfulqa...\n",
      "\n",
      "HGC model fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "# --- THE DEFINITIVE FIX: Load pre-trained model and manually transfer weights ---\n",
    "print('Loading pre-trained GPT-2 model...')\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "print('Initializing custom HGC model architecture...')\n",
    "config = pretrained_model.config\n",
    "hgc_model = GPT2_With_HGC(config)\n",
    "\n",
    "print('Transferring weights to HGC model...')\n",
    "hgc_model.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "print('Weight transfer complete.')\n",
    "\n",
    "# --- Setup Trainer ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "OUTPUT_DIR = 'C:/HGC/models/hgc_gpt2_truthfulqa'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    # THE FIX: Tell the trainer not to remove columns\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hgc_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- Train and Evaluate ---\n",
    "print('Starting fine-tuning of HGC model...')\n",
    "trainer.train()\n",
    "\n",
    "print('\\nEvaluating the HGC model...')\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "perplexity = np.exp(eval_results['eval_loss'])\n",
    "print(f'\\nHGC Model Perplexity: {perplexity:.2f}')\n",
    "\n",
    "print(f'Saving HGC model to {OUTPUT_DIR}...')\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "results_df = pd.DataFrame([{'model': 'HGC-Augmented GPT-2', 'perplexity': perplexity, 'eval_loss': eval_results['eval_loss']}])\n",
    "results_df.to_csv('C:/HGC/data/hgc_results.csv', index=False)\n",
    "print('\\nHGC model fine-tuning complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
